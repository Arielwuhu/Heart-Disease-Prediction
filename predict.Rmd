---
title: "hd"
output:
  pdf_document: default
  html_document: default
date: "2023-04-11"
---

Variable description:

- Age: age of the patient 
  [years]

- Sex: sex of the patient 
  [M: Male, F: Female]

- ChestPainType: chest pain type 
  [TA: Typical Angina, 
  ATA: Atypical Angina, 
  NAP: Non-Anginal Pain, ASY: Asymptomatic]

- RestingBP: resting blood pressure 
  [mm Hg]

- Cholesterol: serum cholesterol 
  [mm/dl]

- FastingBS: fasting blood sugar 
  [1: if FastingBS > 120 mg/dl, 
  0: otherwise]
  
- RestingECG: resting electrocardiogram results 
  [Normal: Normal, 
  ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), 
  LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
  
- MaxHR: maximum heart rate achieved 
  [Numeric value between 60 and 202]

- ExerciseAngina: exercise-induced angina 
  [Y: Yes, N: No]

- Oldpeak: oldpeak = ST 
  [Numeric value measured in depression]

- ST_Slope: the slope of the peak exercise ST segment 
  [Up: upsloping, Flat: flat, Down: downsloping]

- HeartDisease: output class 
  [1: heart disease, 0: Normal]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(corrplot)
library(caret)
library(rpart)
library(randomForest)
library(e1071)
library(glmnet)
library(xgboost)
library(GGally)
```
##########################################
# 1. Data exploration and pre-processing #
##########################################

```{r}
heart_train <- read_csv("heart_train.csv")
heart_test <- read_csv("heart_test_x.csv")
head(heart_train)
```

## Overview

```{r}
# View basic information of the dataset
dim(heart_train)
summary(heart_train)
str(heart_train)
```

```{r}
# Check for missing values
colSums(is.na(heart_train))
```

#######################
# 2. Initial Modeling #
#######################

```{r}
# Function to calculate accuracy
accuracy <- function(y_true, y_pred) {
  return (sum(y_true == y_pred) / length(y_true))
}
```

```{r}
# Split data into train and validation sets
set.seed(123)
train_index <- sample(seq_len(nrow(heart_train)), size = floor(0.8 * nrow(heart_train)))
train_data <- heart_train[train_index, ]
validation_data <- heart_train[-train_index, ]
```

```{r}
set.seed(1)

# Logistic Regression
model_logistic <- glm(HeartDisease ~ ., data = train_data, family = "binomial")
pred_logistic <- predict(model_logistic, newdata = validation_data, type = "response")
MSE_logistic <- mean((validation_data$HeartDisease - pred_logistic)^2)
pred_logistic <- ifelse(pred_logistic >= 0.5, 1, 0)
accuracy_logistic <- accuracy(validation_data$HeartDisease, pred_logistic)

# Decision Tree
model_tree <- rpart(HeartDisease ~ ., data = train_data, method = "class")
pred_tree <- predict(model_tree, newdata = validation_data, type = "class")
accuracy_tree <- accuracy(validation_data$HeartDisease, pred_tree)

# Random Forest
model_rf <- randomForest(as.factor(HeartDisease) ~ ., data = train_data, ntree = 1000)
pred_rf <- predict(model_rf, newdata = validation_data)
accuracy_rf <- accuracy(validation_data$HeartDisease, pred_rf)

# Support Vector Machine
model_svm <- svm(as.factor(HeartDisease) ~ ., data = train_data, kernel = "radial")
pred_svm <- predict(model_svm, newdata = validation_data)
accuracy_svm <- accuracy(validation_data$HeartDisease, pred_svm)

# Display accuracies for all models
cat("Logistic Regression Accuracy:", accuracy_logistic, "\n")
cat("Decision Tree Accuracy:", accuracy_tree, "\n")
cat("Random Forest Accuracy:", accuracy_rf, "\n")
cat("Support Vector Machine Accuracy:", accuracy_svm, "\n")
```

##########################
# 3. Feature Engineering #
##########################

```{r}
# Factorize Fasting BS
heart_train <- heart_train %>%
  mutate_at(vars(Sex, ChestPainType, FastingBS, RestingECG, ExerciseAngina, ST_Slope, HeartDisease), factor)

heart_test <- heart_test %>%
  mutate_at(vars(Sex, ChestPainType, FastingBS, RestingECG, ExerciseAngina, ST_Slope), factor)

heart_Xtrain <- heart_train[-12]
heart_Ytrain <- heart_train[12]

# use sapply to check which columns are numeric
num_cols <- sapply(heart_Xtrain, is.numeric)
num_cols_test <- sapply(heart_test, is.numeric)

# extract the index of the numeric columns
heart_num <- heart_Xtrain[which(num_cols == TRUE)]
heart_num_test <- heart_test[which(num_cols_test == TRUE)]

# extract the index of the categorical columns
heart_cat <- heart_Xtrain[which(num_cols == FALSE)]
heart_cat_test <- heart_test[which(num_cols_test == FALSE)]
```




## 3.1 Target variable - HeartDisease

Good to have balanced classes.

```{r}
heart_train %>% 
  group_by(HeartDisease) %>%
  summarize(count = n(),
            .groups = "drop") %>%
  mutate(total = sum(count),
         prop = count / total) %>%
  ggplot(aes(x = HeartDisease)) +
  geom_bar(aes(y = prop, fill = HeartDisease),
           stat = "identity") +
  ggtitle("Proportion of Heart Disease or Not") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Proportion") +
  xlab("Heart Disease")
```

## 3.2 Numerical Variables

### Check for outliers - no heavy left/right skew, doesn't apply transformation to preserve the interpretability

```{r}
# Check skewness
par(mfrow = c(2, 3))
for (i in names(heart_num)) {
  hist(as.numeric(heart_num[[i]]), xlab=i, main="", cex=0.5)
}
```
```{r}
# Check outlier
par(mfrow = c(2, 3))
for (i in names(heart_num)) {
  boxplot(as.numeric(heart_num[[i]]), xlab=i, main=paste("Boxplot for",i), cex=0.5)
}

```
```{r}
library(reshape2)
# Melt the data to long format
df_long <- gather(heart_cat)

# Create a bar plot for each variable
df_long %>% 
  group_by(key, value) %>% 
  summarize(count = n(),
            .groups = "drop") %>% 
  mutate(total = sum(count),
         prop = count/total) %>% 
  ggplot(aes(x = value, y = prop, fill = value)) +
  geom_bar(stat = "identity") +
  facet_wrap(~key, scales = "free_x") +
  labs(x = "Variable", y = "Proportion") +
  ggtitle("Distribution of Samples in Each Categorical Variable Class")
```

```{r}
lm.fit <- lm(data = heart_train, heart_train$HeartDisease ~ .)
summary(lm.fit)
```


```{r, fig.height=6}
# Create the pair plot
ggpairs(cbind(heart_num, heart_Ytrain),
        mapping = aes(color = HeartDisease, 
                      alpha = 0.5)) 

```


### Creating interaction variables to capture potential non-linear relationships between variables

```{r}
# Add features to training data
heart_num <- data.frame(heart_num)
heart_num$inter.maxhr_age <- heart_num$MaxHR * heart_num$Age
heart_num$inter.oldpeak_age <- heart_num$Oldpeak * heart_num$Age
heart_num$inter.rest_age <- heart_num$RestingBP * heart_num$Age
heart_num$inter.maxhr_chol <- heart_num$MaxHR * heart_num$Cholesterol

# Add features to test data
heart_num_test <- data.frame(heart_num_test)
heart_num_test$inter.maxhr_age <- heart_num_test$MaxHR * heart_num_test$Age
heart_num_test$inter.oldpeak_age <- heart_num_test$Oldpeak * heart_num_test$Age
heart_num_test$inter.rest_age <- heart_num_test$RestingBP * heart_num_test$Age
heart_num_test$inter.maxhr_chol <- heart_num_test$MaxHR * heart_num_test$Cholesterol
```

## 3.3  Categorical Variables

```{r}
# create one-hot encoding for training data
onehot <- dummyVars("~ .", data = heart_cat)

# apply one-hot encoding to categorical data
onehot_cat <- data.frame(predict(onehot, newdata = heart_cat))

# create one-hot encoding for test data
onehot_test <- dummyVars("~ .", data = heart_cat_test)

# apply one-hot encoding to categorical data
onehot_cat_test <- data.frame(predict(onehot_test, newdata = heart_cat_test))
```

## 3.4 **heart_train_v1**: Combine scaled numerical, interaction features and encoded categorical variables as a new training data

Training data:

```{r}
# Combine all variables
heart_train_v1 <- cbind(heart_num, onehot_cat, heart_Ytrain)

### Feature scaling
heart_train_v1$Age <- scale(heart_train_v1$Age)
heart_train_v1$RestingBP <- scale(heart_train_v1$RestingBP)
heart_train_v1$Cholesterol <- scale(heart_train_v1$Cholesterol)
heart_train_v1$RestingBP <- scale(heart_train_v1$MaxHR)
heart_train_v1$Oldpeak <- scale(heart_train_v1$Oldpeak)
heart_train_v1$inter.maxhr_age <- scale(heart_train_v1$inter.maxhr_age)
heart_train_v1$inter.oldpeak_age <- scale(heart_train_v1$inter.oldpeak_age)
heart_train_v1$inter.rest_age <- scale(heart_train_v1$inter.rest_age)
heart_train_v1$inter.maxhr_chol <- scale(heart_train_v1$inter.maxhr_chol)
```

Test data:

```{r}
# Combine all variables
heart_test_v1 <- cbind(heart_num_test, onehot_cat_test)

### Feature scaling
heart_test_v1$Age <- scale(heart_test_v1$Age)
heart_test_v1$RestingBP <- scale(heart_test_v1$RestingBP)
heart_test_v1$Cholesterol <- scale(heart_test_v1$Cholesterol)
heart_test_v1$RestingBP <- scale(heart_test_v1$MaxHR)
heart_test_v1$Oldpeak <- scale(heart_test_v1$Oldpeak)
heart_test_v1$inter.maxhr_age <- scale(heart_test_v1$inter.maxhr_age)
heart_test_v1$inter.oldpeak_age <- scale(heart_test_v1$inter.oldpeak_age)
heart_test_v1$inter.rest_age <- scale(heart_test_v1$inter.rest_age)
heart_test_v1$inter.maxhr_chol <- scale(heart_test_v1$inter.maxhr_chol)
```

########################
# 4. Advanced Modeling #
########################

```{r}
# Split data into train and validation sets
set.seed(123)
train_index <- sample(seq_len(nrow(heart_train_v1)), size = floor(0.8 * nrow(heart_train_v1)))
train_v1 <- heart_train_v1[train_index, ] # featured training data
val_v1 <- heart_train_v1[-train_index, ] # featured test data
```

## Peform LASSO feature selection

```{r}
x <- model.matrix(HeartDisease ~ ., data = train_v1)[, -1]
fit <- cv.glmnet(x, y_train, family = "binomial", alpha = 1)
lasso_xtrain <- train_v1[, which(coef(fit)!=0)]
lasso_xval <- val_v1[, which(coef(fit)!=0)]
```

## 4.1. Start with simple and interpretable model - Logistic Lasso Classification

```{r}
# Build model
logistic_model <- glm(HeartDisease ~ ., data = lasso_xtrain, family = "binomial")
logistic_predictions <- predict(logistic_model, newdata = lasso_xval, type = "response")

# Find best decision boundary
logistic_ypred <- ifelse(predictions >= 0.5, 1, 0)
accuracy_lasso_logistic <- accuracy(y_val, logistic_ypred)

accuracy_lasso_logistic
```

```{r}
logistic_lasso(train_v1, val_v1, y_val)
```

## 4.2. Perform PCA to Improve Logistic Lasso (Warning: prediction from a rank-deficient fit may be misleading)

- PCA: accuracy increases!

```{r}
# Perform PCA to training data
pca <- prcomp(as.matrix(X_train), center = TRUE, scale. = TRUE)
plot(pca$x[,1], pca$x[,2])
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
 
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
 
## now make a fancy looking plot that shows the PCs and the variation:
library(ggplot2)
 
pca.data <- data.frame(Sample=rownames(pca$x),
  X=pca$x[,1],
  Y=pca$x[,2])
pca.data
 
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
  ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
  theme_bw() +
  ggtitle("My PCA Graph")
 
## get the name of the top 10 measurements (genes) that contribute
## most to pc1.
loading_scores <- pca$rotation[,1]
gene_scores <- abs(loading_scores) ## get the magnitudes
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)
top_10_genes <- names(gene_score_ranked[1:10])
 
top_10_genes ## show the names of the top 10 genes
 
pca$rotation[top_10_genes,1] ## show the scores (and +/- sign)
```


```{r}
# Convert sparse matrix to regular matrix
X_train_pca <- as.matrix(predict(pca, X_train)) # only have features, no HeartDisease
X_val_pca <- as.matrix(predict(pca, X_val)) # only have features, no HeartDisease

X_train_pca_df <- data.frame(X_train_pca, HeartDisease = y_train) # have features + HeartDisease
X_val_pca_df <- data.frame(X_val_pca, HeartDisease = y_val) # have features + HeartDisease

# Perform PCA to test data
pca_test <- prcomp(heart_test_v1, center = TRUE, scale. = TRUE)

# Convert sparse matrix to regular matrix
X_test_pca <- as.matrix(predict(pca_test, heart_test_v1)) # only have features, no HeartDisease
```


```{r}
# Fit logistic lasso model on PCA-transformed data
fit <- cv.glmnet(X_train_pca, y_train, family = "binomial", alpha = 1)
lambda_min <- fit$lambda.min
model_logistic_lasso <- glmnet(X_train_pca, y_train, family = "binomial", alpha = 1, lambda = lambda_min)
predictions <- predict(model_logistic_lasso, newx = X_val_pca, type = "response")

# Determine the best decision boundary using accuracy
best_lasso_acc <- 0
for (i in seq(0.1, 0.9, 0.01)) {
  pred_logistic <- ifelse(predictions >= i, 1, 0)
  # accuracy_lasso_logistic <- accuracy(y_val, pred_logistic)
  accuracy_lasso_logistic <- accuracy(y_val, pred_logistic)
  if (best_lasso_acc < accuracy_lasso_logistic) {
    best_lasso_acc <- accuracy_lasso_logistic
    best_y_pred <- pred_logistic
    best_threshold <- i
  }
}
best_lasso_acc
```

**TODO: - Kernel PCA? or other dimension reduction methods?**

### 4.3. Boosting
**TODO: - AdaBoosting; - XGBoost**

##### Fit trees to original dataset

```{r}
set.seed(1)
# Decision Tree
model_tree <- rpart(HeartDisease ~ ., data = train_data, method = "class")
pred_tree <- predict(model_tree, newdata = validation_data, type = "class")
accuracy_tree_orig <- accuracy(validation_data$HeartDisease, pred_tree)
accuracy_tree_orig

# Random Forest
best_rf_acc <- 0
for (tree in seq(100, 1500, 10)) {
  model_rf <- randomForest(as.factor(HeartDisease) ~ ., data = train_data, ntree = tree)
  pred_rf <- predict(model_rf, newdata = validation_data)
  accuracy_rf_feat <- accuracy(y_val, pred_rf)
  if (best_rf_acc < accuracy_rf_feat) {
    best_rf_acc <- accuracy_rf_feat
  }
}
best_rf_acc
```

##### Fit trees to featured dataset - no improvement

```{r}
set.seed(1)
# Decision Tree
model_tree <- rpart(HeartDisease ~ ., data = train_v1, method = "class")
pred_tree <- predict(model_tree, newdata = val_v1, type = "class")
accuracy_tree_feat <- accuracy(y_val, pred_tree)
accuracy_tree_feat

# Random Forest
best_rf_acc <- 0
for (tree in seq(100, 1500, 10)) {
  model_rf <- randomForest(as.factor(HeartDisease) ~ ., data = train_v1, ntree = tree)
  pred_rf <- predict(model_rf, newdata = val_v1)
  accuracy_rf_feat <- accuracy(y_val, pred_rf)
  if (best_rf_acc < accuracy_rf_feat) {
    best_rf_acc <- accuracy_rf_feat
  }
}
best_rf_acc
```

##### Fit trees to pca dataset - decision trees improve a lot

```{r}
set.seed(1)
# Decision Tree
model_tree <- rpart(HeartDisease ~ ., data = X_train_pca_df, method = "class")
pred_tree <- predict(model_tree, newdata = X_val_pca_df, type = "class")
accuracy_tree_pca <- accuracy(y_val, pred_tree)
accuracy_tree_pca

# Random Forest
best_rf_acc <- 0
for (tree in seq(100, 1500, 10)) {
  model_rf <- randomForest(as.factor(HeartDisease) ~ ., data = X_train_pca_df, ntree = tree)
  pred_rf <- predict(model_rf, newdata = X_val_pca_df)
  accuracy_rf_feat <- accuracy(y_val, pred_rf)
  if (best_rf_acc < accuracy_rf_feat) {
    best_rf_acc <- accuracy_rf_feat
  }
}
best_rf_acc
```

##### Ensemble method: XGBoost, decision tree behaves as the base model

```{r}
# Convert data to xgb.DMatrix objects
dtrain <- xgb.DMatrix(data = X_train_pca, label = y_train)

xgb_model <- xgboost(data = dtrain,
                     max.depth = 6, eta = 0.3, nthread = 2,
                     nrounds = 100, objective = "binary:logistic",
                     lambda = 1, alpha = 0)

xgb_train <- xgb.train(data = dtrain,
                       max.depth = 6, eta = 0.3, nthread = 2,
                       nrounds = 100, objective = "binary:logistic",
                       lambda = 1, alpha = 0)

xgb_pred <- predict(xgb_train, newdata = X_val_pca)

# Determine the best decision boundary using accuracy
best_xgb_pca_acc <- 0
for (i in seq(0.1, 0.9, 0.01)) {
  pred_xgb <- ifelse(xgb_pred >= i, 1, 0)
  accuracy_xgb_pca <- accuracy(y_val, pred_xgb)
  if (best_xgb_pca_acc < accuracy_xgb_pca) {
    best_xgb_pca_acc <- accuracy_xgb_pca
    best_y_pred <- pred_xgb
    best_threshold <- i
  }
}
best_xgb_pca_acc

```


--------------------------------------------------------------------------------------
### 4.4. Bagging
**TODO**

### 4.5. 5-fold cross-validation
**TODO: - to original data; - to featured data; - to dimension reduction data**



#################################################
# 6. Actual submission to a prediction contest. #
#################################################

```{r}
# Load test data
heart_test <- read_csv("heart_test_x.csv")
```


```{r}
# Make predictions using the best model
test_predictions <- predict(model_logistic_lasso, newx = X_test_pca, type = "response")
test_pred_result <- ifelse(test_predictions >= best_threshold, 1, 0)

heart.guesses = as.matrix(test_pred_result)
heart.acc = best_lasso_acc
team.name = '2_duck'
save(list=c('heart.guesses','heart.acc','team.name'),file="2_duck_stat462project.RData")
```


